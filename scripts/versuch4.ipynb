{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin\n",
    "from training_data import generate_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainingsdaten generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 0, 0, 1, 0, 0, 1, 0, 0], 0)\n"
     ]
    }
   ],
   "source": [
    "training_data = generate_train_data(10, 3) # 10 pixelbilder generieren welche 3X3 pixel gross sind\n",
    "\n",
    "training_data_sample = training_data[0]\n",
    "training_data_sample_target = [1, 0] if training_data[0][1] == 1 else [0, 1]\n",
    "training_data_sample_image = training_data[0][0]\n",
    "\n",
    "print(training_data_sample) # beispiel einer horizontale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk initialisieren\n",
    "- funktionen definieren\n",
    "- funktionen aufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funktion fÃ¼r fake weights und fake biases eines neurons\n",
    "def init_bias():\n",
    "    bias = random.uniform(-0.5, 0.5)\n",
    "    return bias\n",
    "#print(init_bias())\n",
    "\n",
    "def init_weights(anz_weights):\n",
    "    weights = [random.uniform(-0.5, 0.5) for _ in range(anz_weights)]\n",
    "    return weights\n",
    "#print(init_weights(4))\n",
    "\n",
    "\n",
    "def init_network(dimension):\n",
    "\n",
    "    network = [] # eine liste von listen von dictionaries (jedes neuron wird von einem dicrionary reprÃ¤sentiert)\n",
    "    \n",
    "    # ----- Input layer ----- (hat keine biases oder weights!)\n",
    "    # fÃ¼r jedes layer eine liste machen und diese mit den dictionaries fÃ¼llen\n",
    "    for layer in dimension[:1]: # nur fÃ¼r die input layers\n",
    "        network.append([\n",
    "            {\n",
    "                \"weights\":None, \n",
    "                \"bias\":None, \n",
    "                \"activation\":None\n",
    "            } for _ in range(layer)\n",
    "        ]) # liste mit leeren dictionaries hinzufÃ¼gen fÃ¼r jedes neuron des inputlayers\n",
    "\n",
    "    # ----- Hidden layers -----\n",
    "    # FÃ¼r jedes Hidden-Layer eine Liste mit Dictionaries hinzufÃ¼gen, diese haben weights und biases\n",
    "    for index, layer in enumerate(dimension[1:-1], start=1):  # i startet bei 1, weil wir ab der 2. Schicht zÃ¤hlen\n",
    "        network.append([\n",
    "        {\n",
    "            \"weights\": init_weights(dimension[index - 1]),  # Anzahl Gewichte = Anz. Neuronen im vorherigen Layer\n",
    "            \"bias\": init_bias(),\n",
    "            \"activation\": None\n",
    "        } for _ in range(layer)  # Anzahl Neuronen in der aktuellen Schicht\n",
    "    ])\n",
    "        \n",
    "    # ----- Output layer -----\n",
    "    # FÃ¼r das otput Layer eine Liste mit Dictionaries hinzufÃ¼gen, diese haben weights und biases\n",
    "    for index, layer in enumerate(dimension[-1:], start=-1):  # i startet bei 1, weil wir ab der 2. Schicht zÃ¤hlen\n",
    "        network.append([\n",
    "        {\n",
    "            \"weights\": init_weights(dimension[index - 1]),  # Anzahl Gewichte = Anz. Neuronen im vorherigen Layer\n",
    "            \"bias\": init_bias(),\n",
    "            \"activation\": None\n",
    "        } for _ in range(layer)  # Anzahl Neuronen in der aktuellen Schicht\n",
    "    ])\n",
    "    return network\n",
    "#print(init_network([9, 5, 5, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------- Netzwerk (v.1) vor der Forward-Propagation mit initialisierten b, w -------------------\n",
      "\n",
      "ðŸ”¹ Ebene 0:\n",
      "  â–ª Element 0: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 2: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 3: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 4: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 5: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 6: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 7: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 8: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 1:\n",
      "  â–ª Element 0: {'weights': [-0.2824534203333178, 0.09908103480232111, -0.03324372651804963, 0.43841213892451547, 0.46556681037056247, -0.30705466357616595, 0.2662439944941918, -0.22374140389412234, 0.33918384441842686], 'bias': 0.49454071540659394, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': [-0.324352077183226, 0.29267577684605806, -0.1407937087598281, -0.16923450215085734, -0.4272085467077563, -0.07876799656033351, 0.2804858312611729, -0.3507708567446426, 0.14701504398855392], 'bias': 0.31597159259609875, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 2: {'weights': [-0.40249280987483893, 0.0003289774364876319, 0.47351478229454125, 0.21318902444433463, -0.41031914676421066, -0.22318689004450698, -0.14220629285163788, -0.12078608244055677, 0.47242041046281325], 'bias': -0.4410472184214552, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 3: {'weights': [0.47410068199962874, -0.0896983052494954, 0.06882852356689806, -0.06283903691831683, 0.020860015056701542, 0.42720349766370513, -0.48291689373107727, -0.4458883258478793, -0.40104372771234464], 'bias': 0.03111847179301519, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 4: {'weights': [-0.12880109455521827, -0.1683567073829575, -0.4355372642952433, 0.002523682984472697, 0.01956005949902251, 0.47439412721846674, 0.38274393919681826, 0.032048063652208914, 0.2599689154014818], 'bias': 0.4042299632661439, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 2:\n",
      "  â–ª Element 0: {'weights': [0.04435537696722469, -0.12390960989893773, -0.023723008635488707, 0.32796560326286583, 0.2950700476821675], 'bias': 0.4696465806350131, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': [-0.17918841184957834, -0.11529444063223615, 0.11013079924806835, 0.07494924746309195, 0.25572550880813716], 'bias': 0.4017020824576817, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 2: {'weights': [0.09589114304468183, -0.3013892831775682, -0.28025869790072677, -0.44429681397311616, 0.15456367397204673], 'bias': 0.2860734550463755, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 3: {'weights': [-0.46964254447150733, -0.14210428663452723, -0.3815662945047277, 0.3465555474626638, 0.4488184712121167], 'bias': -0.02894012799446355, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 4: {'weights': [0.2726899362699776, 0.05735849693808437, -0.15442261506758392, 0.060764776515693675, 0.463054764684], 'bias': 0.3936646629326135, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 3:\n",
      "  â–ª Element 0: {'weights': [-0.41225460599519725, -0.1256138607767947, -0.09895249514133575, -0.018533816815582638, 0.48170649101887375], 'bias': -0.05561226454101187, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': [0.06400260341486996, 0.2960189609902194, 0.07688363460385295, 0.2373770790856956, -0.15509857006996175], 'bias': -0.12758896929318364, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_dimension = [9, 5, 5, 2]\n",
    "\n",
    "network1 = init_network(network_dimension) # netzerkt nach dem n-1 ten durchlauf\n",
    "\n",
    "# funktion um das netzwerk schÃ¶ner darzustellen\n",
    "def print_array_structure(array):\n",
    "    for i, layer in enumerate(array):\n",
    "        print(f\"ðŸ”¹ Ebene {i}:\")\n",
    "        for j, element in enumerate(layer):\n",
    "            print(f\"  â–ª Element {j}: {{'weights': {element[\"weights\"]}, 'bias': {element['bias']}, 'activation': {element['activation']}, 'target_activation': {element['activation']}}}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(f\"\\n------------------- Netzwerk (v.1) vor der Forward-Propagation mit initialisierten b, w -------------------\\n\")\n",
    "print_array_structure(network1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation\n",
    "- funktioen definieren\n",
    "  - aktivierungsfunktionen\n",
    "- funktionen aufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6590011388859679\n"
     ]
    }
   ],
   "source": [
    "# aktivierungsfunktionen\n",
    "\n",
    "def activation_relu(x): # hidden layers (werden hier die resultate nicht immer mit jedem layer hÃ¶her?)\n",
    "    return max(0, x)\n",
    "\n",
    "def activation_sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def softmax(x, x_list): # output layer\n",
    "    x_list = [math.exp(i) for i in x_list]\n",
    "    return math.exp(x) / sum(x_list)\n",
    "\n",
    "print(softmax(2.0, [2.0, 1.0, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------- Netzwerk (v.2) nach der Forward-Propagation mit initialisierten a -------------------\n",
      "\n",
      "Input Pixelbild: [1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "ðŸ”¹ Ebene 0:\n",
      "  â–ª Element 0: {'weights': None, 'bias': None, 'activation': 1, 'target_activation': 1}\n",
      "  â–ª Element 1: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 2: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 3: {'weights': None, 'bias': None, 'activation': 1, 'target_activation': 1}\n",
      "  â–ª Element 4: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 5: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 6: {'weights': None, 'bias': None, 'activation': 1, 'target_activation': 1}\n",
      "  â–ª Element 7: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 8: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 1:\n",
      "  â–ª Element 0: {'weights': [-0.2824534203333178, 0.09908103480232111, -0.03324372651804963, 0.43841213892451547, 0.46556681037056247, -0.30705466357616595, 0.2662439944941918, -0.22374140389412234, 0.33918384441842686], 'bias': 0.49454071540659394, 'activation': 0.9167434284919834, 'target_activation': 0.9167434284919834}\n",
      "  â–ª Element 1: {'weights': [-0.324352077183226, 0.29267577684605806, -0.1407937087598281, -0.16923450215085734, -0.4272085467077563, -0.07876799656033351, 0.2804858312611729, -0.3507708567446426, 0.14701504398855392], 'bias': 0.31597159259609875, 'activation': 0.10287084452318829, 'target_activation': 0.10287084452318829}\n",
      "  â–ª Element 2: {'weights': [-0.40249280987483893, 0.0003289774364876319, 0.47351478229454125, 0.21318902444433463, -0.41031914676421066, -0.22318689004450698, -0.14220629285163788, -0.12078608244055677, 0.47242041046281325], 'bias': -0.4410472184214552, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 3: {'weights': [0.47410068199962874, -0.0896983052494954, 0.06882852356689806, -0.06283903691831683, 0.020860015056701542, 0.42720349766370513, -0.48291689373107727, -0.4458883258478793, -0.40104372771234464], 'bias': 0.03111847179301519, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 4: {'weights': [-0.12880109455521827, -0.1683567073829575, -0.4355372642952433, 0.002523682984472697, 0.01956005949902251, 0.47439412721846674, 0.38274393919681826, 0.032048063652208914, 0.2599689154014818], 'bias': 0.4042299632661439, 'activation': 0.6606964908922166, 'target_activation': 0.6606964908922166}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 2:\n",
      "  â–ª Element 0: {'weights': [0.04435537696722469, -0.12390960989893773, -0.023723008635488707, 0.32796560326286583, 0.2950700476821675], 'bias': 0.4696465806350131, 'activation': 0.6925141398441655, 'target_activation': 0.6925141398441655}\n",
      "  â–ª Element 1: {'weights': [-0.17918841184957834, -0.11529444063223615, 0.11013079924806835, 0.07494924746309195, 0.25572550880813716], 'bias': 0.4017020824576817, 'activation': 0.3945287932571618, 'target_activation': 0.3945287932571618}\n",
      "  â–ª Element 2: {'weights': [0.09589114304468183, -0.3013892831775682, -0.28025869790072677, -0.44429681397311616, 0.15456367397204673], 'bias': 0.2860734550463755, 'activation': 0.4450965372051975, 'target_activation': 0.4450965372051975}\n",
      "  â–ª Element 3: {'weights': [-0.46964254447150733, -0.14210428663452723, -0.3815662945047277, 0.3465555474626638, 0.4488184712121167], 'bias': -0.02894012799446355, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 4: {'weights': [0.2726899362699776, 0.05735849693808437, -0.15442261506758392, 0.060764776515693675, 0.463054764684], 'bias': 0.3936646629326135, 'activation': 0.9554905451622545, 'target_activation': 0.9554905451622545}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 3:\n",
      "  â–ª Element 0: {'weights': [-0.41225460599519725, -0.1256138607767947, -0.09895249514133575, -0.018533816815582638, 0.48170649101887375], 'bias': -0.05561226454101187, 'activation': 0.5264783878877657, 'target_activation': 0.5264783878877657}\n",
      "  â–ª Element 1: {'weights': [0.06400260341486996, 0.2960189609902194, 0.07688363460385295, 0.2373770790856956, -0.15509857006996175], 'bias': -0.12758896929318364, 'activation': 0.4735216121122343, 'target_activation': 0.4735216121122343}\n",
      "\n",
      "\n",
      "None\n",
      "Input Pixelbild: [1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "Outputs des letzten Layers: [0.5264783878877657, 0.4735216121122343]\n"
     ]
    }
   ],
   "source": [
    "# forwardpropagation\n",
    "\n",
    "def forward_propagation(pixel_bild, netzwerk): # im pronzip fÃ¼llt diese fuktion das feld \"activation\" des dictinaries!\n",
    "    print(f\"Input Pixelbild: {pixel_bild}\")\n",
    "\n",
    "    new_network = netzwerk\n",
    "\n",
    "    # INPUT-LAYER: den inputwert des inputlayers als activation setzen\n",
    "    for neuron in range(network_dimension[0]):\n",
    "        new_network[0][neuron][\"activation\"] = pixel_bild[neuron] \n",
    "\n",
    "    # HIDDEN-LAYERS und OUTPUT-LAYER: die informationen n-ten layers werden ans n+1-ten layer weitergegeben (fÃ¼r das letzte layer muss eine sigmoid-funktin verwendet werden damit man die klassifiezierung so durchfÃ¼hren kann dass  nÃ¤her bei 1 oder nÃ¤her bei 0 aufteilen kann)\n",
    "    for n in range(1, len(new_network)):\n",
    "        outputs = []\n",
    "\n",
    "        # folgendes wird fÃ¼r jedes neuron eines layers gemacht\n",
    "        for neuron in range(network_dimension[n]):\n",
    "            prev_activations = [new_network[n-1][i][\"activation\"] for i in range(network_dimension[n-1])] # liste mit activations des vorherigen layers\n",
    "            akt_weights = new_network[n][neuron][\"weights\"] # liste mit den weights eines neurons des 2ten layers\n",
    "            akt_bias = new_network[n][neuron][\"bias\"]\n",
    "            \n",
    "            # output-layer\n",
    "            if n == len(new_network)-1:\n",
    "                outputs.append(sum([prev_activations[x] * akt_weights[x] for x in range(len(prev_activations))]) + akt_bias)\n",
    "                if neuron == network_dimension[n]-1:\n",
    "                    pixel_updated_output = [softmax(outputs[output], outputs) for output in range((network_dimension[n]))]\n",
    "                    for sm in range(len(pixel_updated_output)):\n",
    "                        new_network[n][sm][\"activation\"] = pixel_updated_output[sm] # summe aller activations aus dem letzen layer  \n",
    "            \n",
    "            # alle anderen hidden layers\n",
    "            else:\n",
    "                pixel_updated = activation_relu(sum([prev_activations[x] * akt_weights[x] for x in range(len(prev_activations))]) + akt_bias)\n",
    "                new_network[n][neuron][\"activation\"] = pixel_updated # summe aller activations aus dem letzen layer  \n",
    "\n",
    "    return new_network, [node[\"activation\"] for node in new_network[-1]] # mit pixelbild ist hier der semantsche vektor des letzen layers gemeint, also der activations des letzen layers\n",
    "\n",
    "print(f\"\\n------------------- Netzwerk (v.2) nach der Forward-Propagation mit initialisierten a -------------------\\n\")\n",
    "print(print_array_structure(forward_propagation(training_data_sample[0], network1)[0]))\n",
    "print(f\"Outputs des letzten Layers: {(forward_propagation(training_data_sample[0], network1)[1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction machen\n",
    "\n",
    "def predict(image, network):\n",
    "    return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
