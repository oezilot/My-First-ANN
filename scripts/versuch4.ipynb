{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin\n",
    "from training_data import generate_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainingsdaten generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 0, 0, 1, 0, 0, 1, 0, 0], 0)\n"
     ]
    }
   ],
   "source": [
    "training_data = generate_train_data(10, 3) # 10 pixelbilder generieren welche 3X3 pixel gross sind\n",
    "\n",
    "training_data_sample = training_data[0]\n",
    "training_data_sample_target = [1, 0] if training_data[0][1] == 1 else [0, 1]\n",
    "training_data_sample_image = training_data[0][0]\n",
    "\n",
    "print(training_data_sample) # beispiel einer horizontale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk initialisieren\n",
    "- funktionen definieren\n",
    "- funktionen aufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funktion fÃ¼r fake weights und fake biases eines neurons\n",
    "def init_bias():\n",
    "    bias = random.uniform(-0.5, 0.5)\n",
    "    return bias\n",
    "#print(init_bias())\n",
    "\n",
    "def init_weights(anz_weights):\n",
    "    weights = [random.uniform(-0.5, 0.5) for _ in range(anz_weights)]\n",
    "    return weights\n",
    "#print(init_weights(4))\n",
    "\n",
    "\n",
    "def init_network(dimension):\n",
    "\n",
    "    network = [] # eine liste von listen von dictionaries (jedes neuron wird von einem dicrionary reprÃ¤sentiert)\n",
    "    \n",
    "    # ----- Input layer ----- (hat keine biases oder weights!)\n",
    "    # fÃ¼r jedes layer eine liste machen und diese mit den dictionaries fÃ¼llen\n",
    "    for layer in dimension[:1]: # nur fÃ¼r die input layers\n",
    "        network.append([\n",
    "            {\n",
    "                \"weights\":None, \n",
    "                \"bias\":None, \n",
    "                \"activation\":None\n",
    "            } for _ in range(layer)\n",
    "        ]) # liste mit leeren dictionaries hinzufÃ¼gen fÃ¼r jedes neuron des inputlayers\n",
    "\n",
    "    # ----- Hidden layers -----\n",
    "    # FÃ¼r jedes Hidden-Layer eine Liste mit Dictionaries hinzufÃ¼gen, diese haben weights und biases\n",
    "    for index, layer in enumerate(dimension[1:-1], start=1):  # i startet bei 1, weil wir ab der 2. Schicht zÃ¤hlen\n",
    "        network.append([\n",
    "        {\n",
    "            \"weights\": init_weights(dimension[index - 1]),  # Anzahl Gewichte = Anz. Neuronen im vorherigen Layer\n",
    "            \"bias\": init_bias(),\n",
    "            \"activation\": None\n",
    "        } for _ in range(layer)  # Anzahl Neuronen in der aktuellen Schicht\n",
    "    ])\n",
    "        \n",
    "    # ----- Output layer -----\n",
    "    # FÃ¼r das otput Layer eine Liste mit Dictionaries hinzufÃ¼gen, diese haben weights und biases\n",
    "    for index, layer in enumerate(dimension[-1:], start=-1):  # i startet bei 1, weil wir ab der 2. Schicht zÃ¤hlen\n",
    "        network.append([\n",
    "        {\n",
    "            \"weights\": init_weights(dimension[index - 1]),  # Anzahl Gewichte = Anz. Neuronen im vorherigen Layer\n",
    "            \"bias\": init_bias(),\n",
    "            \"activation\": None\n",
    "        } for _ in range(layer)  # Anzahl Neuronen in der aktuellen Schicht\n",
    "    ])\n",
    "    return network\n",
    "#print(init_network([9, 5, 5, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------- Netzwerk (v.1) vor der Forward-Propagation mit initialisierten b, w -------------------\n",
      "\n",
      "ðŸ”¹ Ebene 0:\n",
      "  â–ª Element 0: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 2: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 3: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 4: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 5: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 6: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 7: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 8: {'weights': None, 'bias': None, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 1:\n",
      "  â–ª Element 0: {'weights': [-0.4672426871900478, 0.07246824605487168, -0.1738612933474245, -0.2409447522908802, 0.34305775593164445, -0.04305395866951334, -0.25739115391032485, 0.4136845510444399, 0.14410520047076447], 'bias': 0.45906364987661186, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': [-0.22214351979986036, -0.34107468246973127, -0.23668119914898778, 0.11538599995802279, -0.43586119909732324, -0.19547266182512846, 0.0375474712482563, 0.26085432470338976, -0.1959098787598451], 'bias': 0.4114445658877508, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 2: {'weights': [-0.4956669588127649, 0.29280062727623557, 0.0714756550529545, 0.30912198132559277, -0.04527615172629795, -0.31245341628104895, 0.34436964771934453, 0.32154173055374935, 0.04082995574882553], 'bias': -0.4037746165671332, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 3: {'weights': [-0.11778524469733609, 0.47419894435176857, -0.1861929950550274, -0.3037514875492572, 0.4659379267973155, -0.35279555783310745, 0.09166683860077907, 0.4410019601276376, 0.233582964373808], 'bias': 0.18782697397858317, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 4: {'weights': [-0.07063525928469139, -0.3782342959676108, 0.11344090010562602, 0.3964049707359958, 0.12937435208748116, -0.3751631753846406, 0.1539313225137653, -0.43795999455053614, 0.11925865290662141], 'bias': 0.31711487042679354, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 2:\n",
      "  â–ª Element 0: {'weights': [-0.08475373574549594, 0.2702440379752752, -0.35514868978906444, -0.46001564681059637, -0.27879572841759537], 'bias': 0.49067404943386517, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': [-0.15276956767336602, 0.13097528303187733, 0.250454006602045, -0.03203716243693078, 0.025706853582791123], 'bias': -0.40171806142692845, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 2: {'weights': [-0.20396645735817376, -0.028821450854280672, 0.36292099269024247, -0.48388234657715834, -0.004607851249164963], 'bias': -0.2024499692595364, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 3: {'weights': [0.04904915966315426, 0.47549010324750585, 0.2893740813072465, 0.46329472837405317, -0.0938540916496976], 'bias': -0.49491653365933774, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 4: {'weights': [0.3632744261346619, -0.48922267245396955, -0.08550915212882049, -0.23045223800045056, 0.37440647725037457], 'bias': 0.19333745816261605, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 3:\n",
      "  â–ª Element 0: {'weights': [0.3754168621205217, -0.3803743791971852, -0.47395672695362545, 0.23511466776755974, -0.38916120050038605], 'bias': -0.028773872176945825, 'activation': None, 'target_activation': None}\n",
      "  â–ª Element 1: {'weights': [-0.3133718651773483, -0.259824555046528, 0.4730371938755936, -0.12242493625145401, -0.2517366593352365], 'bias': 0.3500469752265829, 'activation': None, 'target_activation': None}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network_dimension = [9, 5, 5, 2]\n",
    "\n",
    "network1 = init_network(network_dimension) # netzerkt nach dem n-1 ten durchlauf\n",
    "\n",
    "# funktion um das netzwerk schÃ¶ner darzustellen\n",
    "def print_array_structure(array):\n",
    "    for i, layer in enumerate(array):\n",
    "        print(f\"ðŸ”¹ Ebene {i}:\")\n",
    "        for j, element in enumerate(layer):\n",
    "            print(f\"  â–ª Element {j}: {{'weights': {element[\"weights\"]}, 'bias': {element['bias']}, 'activation': {element['activation']}, 'target_activation': {element['activation']}}}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(f\"\\n------------------- Netzwerk (v.1) vor der Forward-Propagation mit initialisierten b, w -------------------\\n\")\n",
    "print_array_structure(network1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation\n",
    "- funktioen definieren\n",
    "  - aktivierungsfunktionen\n",
    "- funktionen aufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6590011388859679\n"
     ]
    }
   ],
   "source": [
    "# aktivierungsfunktionen\n",
    "\n",
    "def activation_relu(x): # hidden layers (werden hier die resultate nicht immer mit jedem layer hÃ¶her?)\n",
    "    return max(0, x)\n",
    "\n",
    "def activation_sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def softmax(x, x_list): # output layer\n",
    "    x_list = [math.exp(i) for i in x_list]\n",
    "    return math.exp(x) / sum(x_list)\n",
    "\n",
    "print(softmax(2.0, [2.0, 1.0, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------- Netzwerk (v.2) nach der Forward-Propagation mit initialisierten a -------------------\n",
      "\n",
      "Input Pixelbild: [1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "ðŸ”¹ Ebene 0:\n",
      "  â–ª Element 0: {'weights': None, 'bias': None, 'activation': 1, 'target_activation': 1}\n",
      "  â–ª Element 1: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 2: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 3: {'weights': None, 'bias': None, 'activation': 1, 'target_activation': 1}\n",
      "  â–ª Element 4: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 5: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 6: {'weights': None, 'bias': None, 'activation': 1, 'target_activation': 1}\n",
      "  â–ª Element 7: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 8: {'weights': None, 'bias': None, 'activation': 0, 'target_activation': 0}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 1:\n",
      "  â–ª Element 0: {'weights': [-0.4672426871900478, 0.07246824605487168, -0.1738612933474245, -0.2409447522908802, 0.34305775593164445, -0.04305395866951334, -0.25739115391032485, 0.4136845510444399, 0.14410520047076447], 'bias': 0.45906364987661186, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 1: {'weights': [-0.22214351979986036, -0.34107468246973127, -0.23668119914898778, 0.11538599995802279, -0.43586119909732324, -0.19547266182512846, 0.0375474712482563, 0.26085432470338976, -0.1959098787598451], 'bias': 0.4114445658877508, 'activation': 0.34223451729416954, 'target_activation': 0.34223451729416954}\n",
      "  â–ª Element 2: {'weights': [-0.4956669588127649, 0.29280062727623557, 0.0714756550529545, 0.30912198132559277, -0.04527615172629795, -0.31245341628104895, 0.34436964771934453, 0.32154173055374935, 0.04082995574882553], 'bias': -0.4037746165671332, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 3: {'weights': [-0.11778524469733609, 0.47419894435176857, -0.1861929950550274, -0.3037514875492572, 0.4659379267973155, -0.35279555783310745, 0.09166683860077907, 0.4410019601276376, 0.233582964373808], 'bias': 0.18782697397858317, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 4: {'weights': [-0.07063525928469139, -0.3782342959676108, 0.11344090010562602, 0.3964049707359958, 0.12937435208748116, -0.3751631753846406, 0.1539313225137653, -0.43795999455053614, 0.11925865290662141], 'bias': 0.31711487042679354, 'activation': 0.7968159043918632, 'target_activation': 0.7968159043918632}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 2:\n",
      "  â–ª Element 0: {'weights': [-0.08475373574549594, 0.2702440379752752, -0.35514868978906444, -0.46001564681059637, -0.27879572841759537], 'bias': 0.49067404943386517, 'activation': 0.36101201684230616, 'target_activation': 0.36101201684230616}\n",
      "  â–ª Element 1: {'weights': [-0.15276956767336602, 0.13097528303187733, 0.250454006602045, -0.03203716243693078, 0.025706853582791123], 'bias': -0.40171806142692845, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 2: {'weights': [-0.20396645735817376, -0.028821450854280672, 0.36292099269024247, -0.48388234657715834, -0.004607851249164963], 'bias': -0.2024499692595364, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 3: {'weights': [0.04904915966315426, 0.47549010324750585, 0.2893740813072465, 0.46329472837405317, -0.0938540916496976], 'bias': -0.49491653365933774, 'activation': 0, 'target_activation': 0}\n",
      "  â–ª Element 4: {'weights': [0.3632744261346619, -0.48922267245396955, -0.08550915212882049, -0.23045223800045056, 0.37440647725037457], 'bias': 0.19333745816261605, 'activation': 0.32424160878639696, 'target_activation': 0.32424160878639696}\n",
      "\n",
      "\n",
      "ðŸ”¹ Ebene 3:\n",
      "  â–ª Element 0: {'weights': [0.3754168621205217, -0.3803743791971852, -0.47395672695362545, 0.23511466776755974, -0.38916120050038605], 'bias': -0.028773872176945825, 'activation': 0.45643112920430334, 'target_activation': 0.45643112920430334}\n",
      "  â–ª Element 1: {'weights': [-0.3133718651773483, -0.259824555046528, 0.4730371938755936, -0.12242493625145401, -0.2517366593352365], 'bias': 0.3500469752265829, 'activation': 0.5435688707956966, 'target_activation': 0.5435688707956966}\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# forwardpropagation\n",
    "\n",
    "def forward_propagation(pixel_bild, netzwerk): # im pronzip fÃ¼llt diese fuktion das feld \"activation\" des dictinaries!\n",
    "    print(f\"Input Pixelbild: {pixel_bild}\")\n",
    "\n",
    "    new_network = netzwerk\n",
    "\n",
    "    # INPUT-LAYER: den inputwert des inputlayers als activation setzen\n",
    "    for neuron in range(network_dimension[0]):\n",
    "        new_network[0][neuron][\"activation\"] = pixel_bild[neuron] \n",
    "\n",
    "    # HIDDEN-LAYERS und OUTPUT-LAYER: die informationen n-ten layers werden ans n+1-ten layer weitergegeben (fÃ¼r das letzte layer muss eine sigmoid-funktin verwendet werden damit man die klassifiezierung so durchfÃ¼hren kann dass  nÃ¤her bei 1 oder nÃ¤her bei 0 aufteilen kann)\n",
    "    for n in range(1, len(new_network)):\n",
    "        outputs = []\n",
    "\n",
    "        # folgendes wird fÃ¼r jedes neuron eines layers gemacht\n",
    "        for neuron in range(network_dimension[n]):\n",
    "            prev_activations = [new_network[n-1][i][\"activation\"] for i in range(network_dimension[n-1])] # liste mit activations des vorherigen layers\n",
    "            akt_weights = new_network[n][neuron][\"weights\"] # liste mit den weights eines neurons des 2ten layers\n",
    "            akt_bias = new_network[n][neuron][\"bias\"]\n",
    "            \n",
    "            # output-layer\n",
    "            if n == len(new_network)-1:\n",
    "                outputs.append(sum([prev_activations[x] * akt_weights[x] for x in range(len(prev_activations))]) + akt_bias)\n",
    "                if neuron == network_dimension[n]-1:\n",
    "                    pixel_updated_output = [softmax(outputs[output], outputs) for output in range((network_dimension[n]))]\n",
    "                    for sm in range(len(pixel_updated_output)):\n",
    "                        new_network[n][sm][\"activation\"] = pixel_updated_output[sm] # summe aller activations aus dem letzen layer  \n",
    "            \n",
    "            # alle anderen hidden layers\n",
    "            else:\n",
    "                pixel_updated = activation_relu(sum([prev_activations[x] * akt_weights[x] for x in range(len(prev_activations))]) + akt_bias)\n",
    "                new_network[n][neuron][\"activation\"] = pixel_updated # summe aller activations aus dem letzen layer  \n",
    "\n",
    "    return new_network, [node[\"activation\"] for node in new_network[-1]] # mit pixelbild ist hier der semantsche vektor des letzen layers gemeint, also der activations des letzen layers\n",
    "\n",
    "print(f\"\\n------------------- Netzwerk (v.2) nach der Forward-Propagation mit initialisierten a -------------------\\n\")\n",
    "print(print_array_structure(forward_propagation(training_data_sample[0], network1)[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
